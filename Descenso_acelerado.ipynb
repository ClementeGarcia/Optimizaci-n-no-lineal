{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSGHMbSeBfWV5/DKeMy8Jm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClementeGarcia/Optimizaci-n-no-lineal/blob/main/Descenso_acelerado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Método de descenso acelerado\n",
        "\n",
        "El método de descenso acelerado es un algoritmo de optimización iterativo utilizado para encontrar el mínimo de una función.\n",
        "\n",
        "**Gradiente**\n",
        "\n",
        "El gradiente de una función $$f(x_1, x_2, ..., x_n)$$ es un vector que apunta en la dirección de mayor crecimeinto de la función. Por lo tanto, $$- \\nabla f$$ apunta en la dirección de mayor descenso.\n",
        "\n",
        "$$\n",
        "\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right)\n",
        "$$\n",
        "En cada iteración, el método actualiza los parámetros moviéndose en la dirección opuesta al gradiente.\n",
        "\n",
        "\n",
        "$$\n",
        "X_{k+1} = X_k - \\lambda \\nabla f(X_k)\n",
        "$$\n",
        "\n",
        "\n",
        "**El algoritmo se detiene cuando: **\n",
        "\n",
        "   Norma del gradiente es pequeña $$\\left(\\| \\nabla f(X_k) \\| < \\varepsilon \\right)$$\n",
        "    \n",
        "  Cambio pequeño en $X$ $$\\left(\\| X_{k+1} - X_k \\| < \\delta \\right)$$\n",
        "     Número máximo de iteraciones alcanzado\n",
        "\n",
        "Vamos a aplicar el método a la siguiente función\n",
        "\n",
        "$$f(x,y) = x - y + 2x^2 + 2xy + y^2 $$\n",
        "\n"
      ],
      "metadata": {
        "id": "0WiR8v6pCq7X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I1PMGyuCKjF",
        "outputId": "93801093-09b3-4511-e131-691a2f3d45f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coordenada x: -0.9999999999999996\n",
            "Coordenada y: 1.4991999999999992\n",
            "Valor de la función: -1.25\n"
          ]
        }
      ],
      "source": [
        "import sympy as sp\n",
        "import numpy as np\n",
        "\n",
        "x, y, lambda_ = sp.symbols('x y lambda')\n",
        "f = x - y + 2*x**2 + 2*x*y + y**2\n",
        "X = np.array([0.0, 0.0])\n",
        "\n",
        "def grad_f(x_val, y_val):\n",
        "    df_dx = sp.diff(f, x)\n",
        "    df_dy = sp.diff(f, y)\n",
        "    return np.array([df_dx.subs({x: x_val, y: y_val}),\n",
        "                     df_dy.subs({x: x_val, y: y_val})], dtype=float)\n",
        "i = 0\n",
        "while True:\n",
        "    S = -grad_f(X[0], X[1])\n",
        "    Y = X + lambda_ * S\n",
        "    g = f.subs({x: Y[0], y: Y[1]})\n",
        "    der_g = sp.diff(g, lambda_)\n",
        "    lambda_opt = sp.solve(der_g, lambda_)[0]\n",
        "    X = X + float(lambda_opt) * S\n",
        "    i += 1\n",
        "    if np.linalg.norm(S) < 0.01:\n",
        "        break\n",
        "print(\"Coordenada x:\", X[0])\n",
        "print(\"Coordenada y:\", X[1])\n",
        "valor_f = float(f.subs({x: X[0], y: X[1]}))\n",
        "print(\"Valor de la función:\", round(valor_f, 4))"
      ]
    }
  ]
}