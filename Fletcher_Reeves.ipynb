{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlGSMHLGVqXIMJ3MKkUv+h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ClementeGarcia/Optimizaci-n-no-lineal/blob/main/Fletcher_Reeves.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Fletcher Reeves\n",
        "\n",
        "El método de Fletcher-Reeves es un algoritmo de optimización no lineal utilizado para minimizar funciones sin restricciones.\n",
        "\n",
        "Dada la función $f(x)$ y su gradiente $\\nabla f(x)$\n",
        "\n",
        "\n",
        "\n",
        "*   Se parte de un punto inicial $x_0$\n",
        "\n",
        "*   La primera dirección de búsqueda es el gradiente negativo:\n",
        "\n",
        "$$ s_0 = -\\nabla f(x_0)$$\n",
        "\n",
        "* Se minimiza $f(x_k + \\lambda s_k)$ con respecto a $\\lambda$\n",
        "\n",
        "* Obtenemos en $\\lambda_k$ óptimo\n",
        "\n",
        "* Se actualiza el punto\n",
        "\n",
        "$$ x_{k+1} = x_k + \\lambda _{k}s_k$$\n",
        "\n",
        "* Calculo del nuevo gradiente\n",
        "\n",
        "$$\\nabla f(x_{k+1})$$\n",
        "\n",
        "\n",
        "\n",
        " $$ \\|\\nabla f(x_{k+1})\\| < \\text{tolerancia}$$ el algoritmo termina.\n",
        "\n",
        "* Cálculo del parámetro $\\beta_k$ (Fletcher-Reeves)\n",
        "$$\n",
        "\\beta_k = \\frac{\\|\\nabla f(x_{k+1})\\|^2}{\\|\\nabla f(x_k)\\|^2}\n",
        "$$\n",
        "\n",
        "* Actualización de la dirección de búsqueda\n",
        "\n",
        "$$\n",
        "s_{k+1} = -\\nabla f(x_{k+1}) + \\beta_k s_k\n",
        "$$\n",
        "\n",
        "\n",
        "Para probar el Método de Fletcher-Reeves utilizaremos la siguiente función\n",
        "\n",
        "$$ f(x,y) = (x-1)^2 + (y - 2)^2$$"
      ],
      "metadata": {
        "id": "GD6o5_MsK9IQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ONzJ1VtJI5X",
        "outputId": "83e3cdca-43d9-412e-aed7-ba1a45b90417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizando la función f(x,y) = (x-1)² + (y-2)²\n",
            "Punto inicial: [ 5. -3.]\n",
            "\n",
            "\t No hay beta\n",
            "\n",
            "Punto óptimo encontrado: [1. 2.]\n",
            "Valor de la función en el óptimo: 1.5777218104420236e-30\n",
            "Número total de iteraciones: 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize_scalar\n",
        "\n",
        "def fletcher(f, grad_f, x0, tol=1e-6, max_iter=1000):\n",
        "    history = [x0.copy()]\n",
        "    x = x0.copy()\n",
        "    n = len(x0)\n",
        "\n",
        "    grad_c = grad_f(x)\n",
        "    s = -grad_c\n",
        "    grad_p = grad_c.copy()\n",
        "\n",
        "\n",
        "    for i in range(1, max_iter + 1):\n",
        "        def f_lambda(lambda_val):\n",
        "            return f(x + lambda_val * s)\n",
        "\n",
        "        res = minimize_scalar(f_lambda)\n",
        "        lambda_opt = res.x\n",
        "        x = x + lambda_opt * s\n",
        "        history.append(x.copy())\n",
        "        grad_c = grad_f(x)\n",
        "        grad_norm = np.linalg.norm(grad_c)\n",
        "\n",
        "        if i > 1:\n",
        "            beta = np.dot(grad_c, grad_c) / np.dot(grad_p, grad_p)\n",
        "            print(f\"\\t{beta:.6f}\")\n",
        "        else:\n",
        "            print(\"\\t No hay beta\")\n",
        "\n",
        "        if grad_norm < tol:\n",
        "            break\n",
        "        if i < max_iter:\n",
        "            beta = np.dot(grad_c, grad_c) / np.dot(grad_p, grad_p)\n",
        "            s = -grad_c + beta * s\n",
        "            grad_p = grad_c.copy()\n",
        "\n",
        "    return x, history\n",
        "\n",
        "def f(x):\n",
        "    return (x[0]-1)**2 + (x[1]-2)**2\n",
        "\n",
        "def grad_f(x):\n",
        "    return np.array([2*(x[0]-1), 2*(x[1]-2)])\n",
        "\n",
        "x0 = np.array([5.0, -3.0])\n",
        "\n",
        "print(\"Optimizando la función f(x,y) = (x-1)² + (y-2)²\")\n",
        "print(f\"Punto inicial: {x0}\\n\")\n",
        "\n",
        "x_opt, history = fletcher(f, grad_f, x0)\n",
        "\n",
        "print(f\"\\nPunto óptimo encontrado: {x_opt}\")\n",
        "print(f\"Valor de la función en el óptimo: {f(x_opt)}\")\n",
        "print(f\"Número total de iteraciones: {len(history)-1}\")"
      ]
    }
  ]
}